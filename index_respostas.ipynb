{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "!gzip -d NASA_access_log_Aug95.gz >/dev/null \n",
    "!gzip -d NASA_access_log_Jul95.gz >/dev/null\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copiando os dados para o HDFS\n",
    "Caminho para os arquivos de entrada: /user/jovyan/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/jovyan/input\n",
    "!hdfs dfs -put NASA_access_log_Aug95 /user/jovyan/input\n",
    "!hdfs dfs -put NASA_access_log_Jul95 /user/jovyan/input\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando o Script Spark - Nasa log analysis\n",
    "### Informações obtidas:\n",
    "#### 1. Número de hosts únicos.\n",
    "#### 2. O total de erros 404.\n",
    "#### 3. As 5 URLs que mais causaram erro 404.\n",
    "#### 4. Quantidade de erros 404 por dia.\n",
    "#### 5. O total de bytes retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/08 02:50:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "19/11/08 02:50:17 INFO spark.SparkContext: Running Spark version 2.4.4\n",
      "19/11/08 02:50:17 INFO spark.SparkContext: Submitted application: Nasa\n",
      "19/11/08 02:50:17 INFO spark.SecurityManager: Changing view acls to: jovyan\n",
      "19/11/08 02:50:17 INFO spark.SecurityManager: Changing modify acls to: jovyan\n",
      "19/11/08 02:50:17 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "19/11/08 02:50:17 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "19/11/08 02:50:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "19/11/08 02:50:17 INFO util.Utils: Successfully started service 'sparkDriver' on port 39431.\n",
      "19/11/08 02:50:17 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "19/11/08 02:50:17 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "19/11/08 02:50:17 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/11/08 02:50:17 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/11/08 02:50:17 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1b79a533-ad58-44a1-858e-d7dde3f10a13\n",
      "19/11/08 02:50:17 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/11/08 02:50:17 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "19/11/08 02:50:17 INFO util.log: Logging initialized @3452ms\n",
      "19/11/08 02:50:17 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "19/11/08 02:50:18 INFO server.Server: Started @3559ms\n",
      "19/11/08 02:50:18 INFO server.AbstractConnector: Started ServerConnector@3d868c80{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "19/11/08 02:50:18 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d7a3ade{/jobs,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11733940{/jobs/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4eb8a67{/jobs/job,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24fbf6d0{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c4a6236{/stages,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@742a7d6a{/stages/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15d698de{/stages/stage,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36a2db95{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1713b5f7{/stages/pool,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c96e866{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c4d79fb{/storage,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c936d94{/storage/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66a5d1fc{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dbbc096{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a9ee67b{/environment,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d96c571{/environment/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3574d10e{/executors,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46380846{/executors/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3be893c4{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@709e6485{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63712739{/static,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27dc2be8{/,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@305d42a6{/api,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a284c2c{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d28871f{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:18 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:4040\n",
      "19/11/08 02:50:18 INFO executor.Executor: Starting executor ID driver on host localhost\n",
      "19/11/08 02:50:18 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44147.\n",
      "19/11/08 02:50:18 INFO netty.NettyBlockTransferService: Server created on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147\n",
      "19/11/08 02:50:18 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/11/08 02:50:18 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5, 44147, None)\n",
      "19/11/08 02:50:18 INFO storage.BlockManagerMasterEndpoint: Registering block manager jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 with 366.3 MB RAM, BlockManagerId(driver, jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5, 44147, None)\n",
      "19/11/08 02:50:18 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5, 44147, None)\n",
      "19/11/08 02:50:18 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5, 44147, None)\n",
      "19/11/08 02:50:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@321dafb3{/metrics/json,null,AVAILABLE,@Spark}\n",
      "19/11/08 02:50:19 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 239.6 KB, free 366.1 MB)\n",
      "19/11/08 02:50:19 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.0 KB, free 366.0 MB)\n",
      "19/11/08 02:50:19 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 23.0 KB, free: 366.3 MB)\n",
      "19/11/08 02:50:19 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "19/11/08 02:50:20 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "19/11/08 02:50:20 INFO spark.SparkContext: Starting job: count at /home/jovyan/nasa_requests.py:90\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Registering RDD 3 (distinct at /home/jovyan/nasa_requests.py:90)\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Got job 0 (count at /home/jovyan/nasa_requests.py:90) with 4 output partitions\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at /home/jovyan/nasa_requests.py:90)\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /home/jovyan/nasa_requests.py:90), which has no missing parents\n",
      "19/11/08 02:50:20 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.8 KB, free 366.0 MB)\n",
      "19/11/08 02:50:20 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.4 KB, free 366.0 MB)\n",
      "19/11/08 02:50:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 8.4 KB, free: 366.3 MB)\n",
      "19/11/08 02:50:20 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:50:20 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at distinct at /home/jovyan/nasa_requests.py:90) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:50:20 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 4 tasks\n",
      "19/11/08 02:50:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7910 bytes)\n",
      "19/11/08 02:50:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7910 bytes)\n",
      "19/11/08 02:50:20 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/11/08 02:50:20 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "19/11/08 02:50:20 INFO rdd.HadoopRDD: Input split: hdfs://localhost:9000/user/jovyan/input/NASA_access_log_Aug95:134217728+33596042\n",
      "19/11/08 02:50:20 INFO rdd.HadoopRDD: Input split: hdfs://localhost:9000/user/jovyan/input/NASA_access_log_Aug95:0+134217728\n",
      "19/11/08 02:50:23 INFO memory.MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 7.4 MB, free 358.6 MB)\n",
      "19/11/08 02:50:23 INFO storage.BlockManagerInfo: Added rdd_1_1 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 7.4 MB, free: 358.9 MB)\n",
      "19/11/08 02:50:26 INFO memory.MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 29.5 MB, free 329.1 MB)\n",
      "19/11/08 02:50:26 INFO storage.BlockManagerInfo: Added rdd_1_0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 29.5 MB, free: 329.3 MB)\n",
      "19/11/08 02:50:31 INFO python.PythonRunner: Times: total = 8192, boot = 1450, init = 48, finish = 6694\n",
      "19/11/08 02:50:31 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1809 bytes result sent to driver\n",
      "19/11/08 02:50:31 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 7910 bytes)\n",
      "19/11/08 02:50:31 INFO executor.Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "19/11/08 02:50:31 INFO rdd.HadoopRDD: Input split: hdfs://localhost:9000/user/jovyan/input/NASA_access_log_Jul95:0+134217728\n",
      "19/11/08 02:50:31 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 11262 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:50:31 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51877\n",
      "19/11/08 02:50:35 INFO memory.MemoryStore: Block rdd_1_2 stored as values in memory (estimated size 30.6 MB, free 298.4 MB)\n",
      "19/11/08 02:50:35 INFO storage.BlockManagerInfo: Added rdd_1_2 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 30.6 MB, free: 298.7 MB)\n",
      "19/11/08 02:50:52 INFO python.PythonRunner: Times: total = 26046, boot = 9, init = 5, finish = 26032\n",
      "19/11/08 02:50:52 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1809 bytes result sent to driver\n",
      "19/11/08 02:50:52 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 7910 bytes)\n",
      "19/11/08 02:50:52 INFO executor.Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "19/11/08 02:50:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 31706 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:50:52 INFO rdd.HadoopRDD: Input split: hdfs://localhost:9000/user/jovyan/input/NASA_access_log_Jul95:134217728+71024640\n",
      "19/11/08 02:50:54 INFO memory.MemoryStore: Block rdd_1_3 stored as values in memory (estimated size 15.9 MB, free 282.5 MB)\n",
      "19/11/08 02:50:54 INFO storage.BlockManagerInfo: Added rdd_1_3 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 15.9 MB, free: 282.8 MB)\n",
      "19/11/08 02:51:00 INFO python.PythonRunner: Times: total = 25296, boot = -3227, init = 3231, finish = 25292\n",
      "19/11/08 02:51:00 INFO executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 1809 bytes result sent to driver\n",
      "19/11/08 02:51:00 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 28557 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:51:05 INFO python.PythonRunner: Times: total = 11162, boot = -1711, init = 1714, finish = 11159\n",
      "19/11/08 02:51:05 INFO executor.Executor: Finished task 3.0 in stage 0.0 (TID 3). 1809 bytes result sent to driver\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 12882 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (distinct at /home/jovyan/nasa_requests.py:90) finished in 44.760 s\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at count at /home/jovyan/nasa_requests.py:90), which has no missing parents\n",
      "19/11/08 02:51:05 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.2 KB, free 282.5 MB)\n",
      "19/11/08 02:51:05 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KB, free 282.5 MB)\n",
      "19/11/08 02:51:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 5.8 KB, free: 282.8 MB)\n",
      "19/11/08 02:51:05 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (PythonRDD[6] at count at /home/jovyan/nasa_requests.py:90) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 4 tasks\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, localhost, executor driver, partition 1, ANY, 7662 bytes)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 4)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 5)\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms\n",
      "19/11/08 02:51:05 INFO python.PythonRunner: Times: total = 175, boot = -154, init = 173, finish = 156\n",
      "19/11/08 02:51:05 INFO python.PythonRunner: Times: total = 177, boot = -4989, init = 5009, finish = 157\n",
      "19/11/08 02:51:05 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 5). 1720 bytes result sent to driver\n",
      "19/11/08 02:51:05 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 4). 1720 bytes result sent to driver\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, localhost, executor driver, partition 2, ANY, 7662 bytes)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 6)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, localhost, executor driver, partition 3, ANY, 7662 bytes)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 7)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 292 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 289 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:51:05 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:51:05 INFO python.PythonRunner: Times: total = 59, boot = -76, init = 82, finish = 53\n",
      "19/11/08 02:51:05 INFO python.PythonRunner: Times: total = 99, boot = -75, init = 82, finish = 92\n",
      "19/11/08 02:51:05 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 7). 1763 bytes result sent to driver\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 174 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 6). 1720 bytes result sent to driver\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 178 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: ResultStage 1 (count at /home/jovyan/nasa_requests.py:90) finished in 0.477 s\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Job 0 finished: count at /home/jovyan/nasa_requests.py:90, took 45.379017 s\n",
      "19/11/08 02:51:05 INFO spark.SparkContext: Starting job: count at /home/jovyan/nasa_requests.py:93\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Got job 1 (count at /home/jovyan/nasa_requests.py:93) with 4 output partitions\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (count at /home/jovyan/nasa_requests.py:93)\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home/jovyan/nasa_requests.py:93), which has no missing parents\n",
      "19/11/08 02:51:05 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.0 KB, free 282.5 MB)\n",
      "19/11/08 02:51:05 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KB, free 282.5 MB)\n",
      "19/11/08 02:51:05 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 6.5 KB, free: 282.8 MB)\n",
      "19/11/08 02:51:05 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:51:05 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home/jovyan/nasa_requests.py:93) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 4 tasks\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:51:05 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 8)\n",
      "19/11/08 02:51:05 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 9)\n",
      "19/11/08 02:51:05 INFO storage.BlockManager: Found block rdd_1_1 locally\n",
      "19/11/08 02:51:05 INFO storage.BlockManager: Found block rdd_1_0 locally\n",
      "19/11/08 02:51:06 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 5.8 KB, free: 282.8 MB)\n",
      "19/11/08 02:51:11 INFO python.PythonRunner: Times: total = 5887, boot = -177, init = 182, finish = 5882\n",
      "19/11/08 02:51:11 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 9). 1548 bytes result sent to driver\n",
      "19/11/08 02:51:11 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:51:11 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 10)\n",
      "19/11/08 02:51:11 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 9) in 5915 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:51:11 INFO storage.BlockManager: Found block rdd_1_2 locally\n",
      "19/11/08 02:51:28 INFO python.PythonRunner: Times: total = 22996, boot = -133, init = 139, finish = 22990\n",
      "19/11/08 02:51:28 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 8). 1548 bytes result sent to driver\n",
      "19/11/08 02:51:28 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:51:28 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 11)\n",
      "19/11/08 02:51:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 23024 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:51:28 INFO storage.BlockManager: Found block rdd_1_3 locally\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 10\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 49\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 12\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 37\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 16\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 9\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 45\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 46\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 40\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 34\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 26\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 17\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 29\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 22\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 18\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 48\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 38\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 36\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 39\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 25\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 19\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 30\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 43\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 31\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 47\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 32\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 50\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 5\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 15\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 33\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 21\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 6\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 24\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 23\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 3\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 28\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 42\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 8\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 27\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 13\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 2\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 41\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 14\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 4\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 11\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 1\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 7\n",
      "19/11/08 02:51:32 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 8.4 KB, free: 282.8 MB)\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 35\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 20\n",
      "19/11/08 02:51:32 INFO spark.ContextCleaner: Cleaned accumulator 44\n",
      "19/11/08 02:51:35 INFO python.PythonRunner: Times: total = 23171, boot = -64, init = 67, finish = 23168\n",
      "19/11/08 02:51:35 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 10). 1548 bytes result sent to driver\n",
      "19/11/08 02:51:35 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 10) in 23266 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:51:39 INFO python.PythonRunner: Times: total = 10552, boot = -57, init = 62, finish = 10547\n",
      "19/11/08 02:51:39 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 11). 1548 bytes result sent to driver\n",
      "19/11/08 02:51:39 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 11) in 10611 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:51:39 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: ResultStage 2 (count at /home/jovyan/nasa_requests.py:93) finished in 33.644 s\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Job 1 finished: count at /home/jovyan/nasa_requests.py:93, took 33.651325 s\n",
      "19/11/08 02:51:39 INFO spark.SparkContext: Starting job: sortByKey at /home/jovyan/nasa_requests.py:98\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Registering RDD 10 (reduceByKey at /home/jovyan/nasa_requests.py:97)\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Got job 2 (sortByKey at /home/jovyan/nasa_requests.py:98) with 4 output partitions\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (sortByKey at /home/jovyan/nasa_requests.py:98)\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (PairwiseRDD[10] at reduceByKey at /home/jovyan/nasa_requests.py:97), which has no missing parents\n",
      "19/11/08 02:51:39 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.7 KB, free 282.5 MB)\n",
      "19/11/08 02:51:39 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.4 KB, free 282.5 MB)\n",
      "19/11/08 02:51:39 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 8.4 KB, free: 282.8 MB)\n",
      "19/11/08 02:51:39 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:51:39 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (PairwiseRDD[10] at reduceByKey at /home/jovyan/nasa_requests.py:97) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:51:39 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 4 tasks\n",
      "19/11/08 02:51:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7910 bytes)\n",
      "19/11/08 02:51:39 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 7910 bytes)\n",
      "19/11/08 02:51:39 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 12)\n",
      "19/11/08 02:51:39 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 13)\n",
      "19/11/08 02:51:39 INFO storage.BlockManager: Found block rdd_1_0 locally\n",
      "19/11/08 02:51:39 INFO storage.BlockManager: Found block rdd_1_1 locally\n",
      "19/11/08 02:51:45 INFO python.PythonRunner: Times: total = 5910, boot = -89, init = 92, finish = 5907\n",
      "19/11/08 02:51:45 INFO memory.MemoryStore: Block rdd_8_1 stored as values in memory (estimated size 42.5 KB, free 282.5 MB)\n",
      "19/11/08 02:51:45 INFO storage.BlockManagerInfo: Added rdd_8_1 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 42.5 KB, free: 282.7 MB)\n",
      "19/11/08 02:51:45 INFO python.PythonRunner: Times: total = 12, boot = -36, init = 38, finish = 10\n",
      "19/11/08 02:51:45 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 13). 1766 bytes result sent to driver\n",
      "19/11/08 02:51:45 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 7910 bytes)\n",
      "19/11/08 02:51:45 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 14)\n",
      "19/11/08 02:51:45 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 13) in 5998 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:51:45 INFO storage.BlockManager: Found block rdd_1_2 locally\n",
      "19/11/08 02:52:03 INFO python.PythonRunner: Times: total = 24206, boot = -4542, init = 4545, finish = 24203\n",
      "19/11/08 02:52:03 INFO memory.MemoryStore: Block rdd_8_0 stored as values in memory (estimated size 175.1 KB, free 282.3 MB)\n",
      "19/11/08 02:52:03 INFO storage.BlockManagerInfo: Added rdd_8_0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 175.1 KB, free: 282.5 MB)\n",
      "19/11/08 02:52:03 INFO python.PythonRunner: Times: total = 17, boot = -53, init = 56, finish = 14\n",
      "19/11/08 02:52:03 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 12). 1766 bytes result sent to driver\n",
      "19/11/08 02:52:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 7910 bytes)\n",
      "19/11/08 02:52:03 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 15)\n",
      "19/11/08 02:52:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 12) in 24376 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:52:03 INFO storage.BlockManager: Found block rdd_1_3 locally\n",
      "19/11/08 02:52:08 INFO python.PythonRunner: Times: total = 22701, boot = -20, init = 23, finish = 22698\n",
      "19/11/08 02:52:08 INFO memory.MemoryStore: Block rdd_8_2 stored as values in memory (estimated size 159.0 KB, free 282.1 MB)\n",
      "19/11/08 02:52:08 INFO storage.BlockManagerInfo: Added rdd_8_2 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 159.0 KB, free: 282.4 MB)\n",
      "19/11/08 02:52:08 INFO python.PythonRunner: Times: total = 68, boot = 0, init = 2, finish = 66\n",
      "19/11/08 02:52:08 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 14). 1766 bytes result sent to driver\n",
      "19/11/08 02:52:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 14) in 22804 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:52:13 INFO python.PythonRunner: Times: total = 9963, boot = -89, init = 92, finish = 9960\n",
      "19/11/08 02:52:13 INFO memory.MemoryStore: Block rdd_8_3 stored as values in memory (estimated size 95.2 KB, free 282.0 MB)\n",
      "19/11/08 02:52:13 INFO storage.BlockManagerInfo: Added rdd_8_3 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 95.2 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 18, boot = -5611, init = 5613, finish = 16\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 15). 1766 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 15) in 10068 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (reduceByKey at /home/jovyan/nasa_requests.py:97) finished in 34.455 s\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (PythonRDD[13] at sortByKey at /home/jovyan/nasa_requests.py:98), which has no missing parents\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.2 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 5.9 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (PythonRDD[13] at sortByKey at /home/jovyan/nasa_requests.py:98) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 4 tasks\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17, localhost, executor driver, partition 1, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 17)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 16)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 8, boot = -125, init = 130, finish = 3\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 17). 1720 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 18, localhost, executor driver, partition 2, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 18)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 23 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 51, boot = 0, init = 49, finish = 2\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 79, boot = -53, init = 97, finish = 35\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 18). 1763 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 16). 1720 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 19, localhost, executor driver, partition 3, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 19)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 18) in 73 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 95 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 47, boot = 0, init = 44, finish = 3\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 19). 1720 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 102 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: ResultStage 4 (sortByKey at /home/jovyan/nasa_requests.py:98) finished in 0.219 s\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Job 2 finished: sortByKey at /home/jovyan/nasa_requests.py:98, took 34.686876 s\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Starting job: sortByKey at /home/jovyan/nasa_requests.py:98\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Got job 3 (sortByKey at /home/jovyan/nasa_requests.py:98) with 4 output partitions\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (sortByKey at /home/jovyan/nasa_requests.py:98)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (PythonRDD[14] at sortByKey at /home/jovyan/nasa_requests.py:98), which has no missing parents\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.0 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 5.8 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 6 (PythonRDD[14] at sortByKey at /home/jovyan/nasa_requests.py:98) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 4 tasks\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 20, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 21, localhost, executor driver, partition 1, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 20)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 21)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 9, boot = -105, init = 110, finish = 4\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 21). 1833 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 22, localhost, executor driver, partition 2, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 22)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 21) in 24 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 51, boot = -201, init = 247, finish = 5\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 63, boot = -1, init = 44, finish = 20\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 20). 1876 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 22). 1820 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 23, localhost, executor driver, partition 3, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 23)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 20) in 98 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 22) in 75 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 49, boot = -2, init = 47, finish = 4\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 23). 1829 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 23) in 62 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: ResultStage 6 (sortByKey at /home/jovyan/nasa_requests.py:98) finished in 0.215 s\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Job 3 finished: sortByKey at /home/jovyan/nasa_requests.py:98, took 0.222549 s\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Registering RDD 16 (sortByKey at /home/jovyan/nasa_requests.py:98)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Got job 4 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at PythonRDD.scala:153)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (PairwiseRDD[16] at sortByKey at /home/jovyan/nasa_requests.py:98), which has no missing parents\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.8 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.5 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 6.5 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 8 (PairwiseRDD[16] at sortByKey at /home/jovyan/nasa_requests.py:98) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 4 tasks\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, localhost, executor driver, partition 0, ANY, 7651 bytes)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, localhost, executor driver, partition 1, ANY, 7651 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 24)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 25)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 91, boot = -149, init = 234, finish = 6\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 91, boot = -90, init = 175, finish = 6\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 25). 1938 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 24). 1938 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, localhost, executor driver, partition 2, ANY, 7651 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 26)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 27, localhost, executor driver, partition 3, ANY, 7651 bytes)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 121 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 27)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 121 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 48, boot = -47, init = 89, finish = 6\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 3.0 in stage 8.0 (TID 27). 1938 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 49, boot = -45, init = 89, finish = 5\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 8.0 (TID 27) in 134 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 2.0 in stage 8.0 (TID 26). 1938 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 139 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (sortByKey at /home/jovyan/nasa_requests.py:98) finished in 0.270 s\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 9)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (PythonRDD[19] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.1 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.6 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 4.6 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[19] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 28, localhost, executor driver, partition 0, ANY, 7662 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 28)\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/08 02:52:14 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/08 02:52:14 INFO python.PythonRunner: Times: total = 45, boot = -65, init = 109, finish = 1\n",
      "19/11/08 02:52:14 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 28). 1991 bytes result sent to driver\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 28) in 80 ms on localhost (executor driver) (1/1)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:153) finished in 0.090 s\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:153, took 0.372086 s\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Starting job: countByKey at /home/jovyan/nasa_requests.py:101\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Got job 5 (countByKey at /home/jovyan/nasa_requests.py:101) with 4 output partitions\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at /home/jovyan/nasa_requests.py:101)\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (PythonRDD[20] at countByKey at /home/jovyan/nasa_requests.py:101), which has no missing parents\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 10.3 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.8 KB, free 282.0 MB)\n",
      "19/11/08 02:52:14 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 6.8 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:14 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:52:14 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (PythonRDD[20] at countByKey at /home/jovyan/nasa_requests.py:101) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 4 tasks\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 30, localhost, executor driver, partition 1, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 29)\n",
      "19/11/08 02:52:14 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 30)\n",
      "19/11/08 02:52:14 INFO storage.BlockManager: Found block rdd_1_0 locally\n",
      "19/11/08 02:52:14 INFO storage.BlockManager: Found block rdd_1_1 locally\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 190\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 199\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 149\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 93\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 97\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 116\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 78\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 87\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 118\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 83\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 160\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 197\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 129\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 169\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 179\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 145\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 170\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 187\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 85\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 127\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 195\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 95\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 117\n",
      "19/11/08 02:52:15 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 4.6 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 96\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 175\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 148\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 174\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 121\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 156\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 100\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 180\n",
      "19/11/08 02:52:15 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 5.8 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 189\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 140\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 182\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 76\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 166\n",
      "19/11/08 02:52:15 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 8.4 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 146\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 107\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 178\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 168\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 172\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 200\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 147\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 84\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 122\n",
      "19/11/08 02:52:15 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 5.9 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 191\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 164\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 152\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 154\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 157\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 115\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 104\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 113\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 193\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 89\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 137\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 109\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 101\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 82\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 177\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 183\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 126\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 132\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 158\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 88\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 186\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 120\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 90\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 192\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 196\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 110\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 176\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 143\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 161\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 112\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 134\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 103\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 125\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 136\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 108\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 114\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 80\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 94\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 198\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 167\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 163\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 92\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 173\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 188\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 165\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 111\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 194\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 184\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 155\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 142\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 130\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 144\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 77\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 138\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 185\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 162\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 153\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 150\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 105\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 131\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 79\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 91\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 135\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 151\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 102\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 81\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 133\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 141\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 171\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 106\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 98\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 119\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 86\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 181\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 123\n",
      "19/11/08 02:52:15 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 6.5 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 139\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 99\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 159\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 124\n",
      "19/11/08 02:52:15 INFO spark.ContextCleaner: Cleaned accumulator 128\n",
      "19/11/08 02:52:21 INFO python.PythonRunner: Times: total = 6367, boot = -47, init = 96, finish = 6318\n",
      "19/11/08 02:52:21 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 30). 1700 bytes result sent to driver\n",
      "19/11/08 02:52:21 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 31, localhost, executor driver, partition 2, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:21 INFO executor.Executor: Running task 2.0 in stage 10.0 (TID 31)\n",
      "19/11/08 02:52:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 30) in 6378 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:52:21 INFO storage.BlockManager: Found block rdd_1_2 locally\n",
      "19/11/08 02:52:39 INFO python.PythonRunner: Times: total = 24075, boot = -192, init = 241, finish = 24026\n",
      "19/11/08 02:52:39 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 29). 2139 bytes result sent to driver\n",
      "19/11/08 02:52:39 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 10.0 (TID 32, localhost, executor driver, partition 3, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:39 INFO executor.Executor: Running task 3.0 in stage 10.0 (TID 32)\n",
      "19/11/08 02:52:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 29) in 24093 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:52:39 INFO storage.BlockManager: Found block rdd_1_3 locally\n",
      "19/11/08 02:52:44 INFO python.PythonRunner: Times: total = 23526, boot = -3, init = 6, finish = 23523\n",
      "19/11/08 02:52:44 INFO executor.Executor: Finished task 2.0 in stage 10.0 (TID 31). 1954 bytes result sent to driver\n",
      "19/11/08 02:52:44 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 31) in 23576 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:52:49 INFO python.PythonRunner: Times: total = 10654, boot = -11, init = 14, finish = 10651\n",
      "19/11/08 02:52:49 INFO executor.Executor: Finished task 3.0 in stage 10.0 (TID 32). 1847 bytes result sent to driver\n",
      "19/11/08 02:52:49 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 10.0 (TID 32) in 10670 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:52:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at /home/jovyan/nasa_requests.py:101) finished in 34.770 s\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Job 5 finished: countByKey at /home/jovyan/nasa_requests.py:101, took 34.774786 s\n",
      "19/11/08 02:52:49 INFO spark.SparkContext: Starting job: reduce at /home/jovyan/nasa_requests.py:103\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Got job 6 (reduce at /home/jovyan/nasa_requests.py:103) with 4 output partitions\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (reduce at /home/jovyan/nasa_requests.py:103)\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (PythonRDD[21] at reduce at /home/jovyan/nasa_requests.py:103), which has no missing parents\n",
      "19/11/08 02:52:49 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.7 KB, free 282.0 MB)\n",
      "19/11/08 02:52:49 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.7 KB, free 282.0 MB)\n",
      "19/11/08 02:52:49 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 (size: 5.7 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:49 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/08 02:52:49 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 11 (PythonRDD[21] at reduce at /home/jovyan/nasa_requests.py:103) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "19/11/08 02:52:49 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 4 tasks\n",
      "19/11/08 02:52:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 34, localhost, executor driver, partition 1, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:49 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 33)\n",
      "19/11/08 02:52:49 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 34)\n",
      "19/11/08 02:52:49 INFO storage.BlockManager: Found block rdd_1_1 locally\n",
      "19/11/08 02:52:49 INFO storage.BlockManager: Found block rdd_1_0 locally\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 214\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 222\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 215\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 216\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 206\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 202\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 209\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 219\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 201\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 207\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 204\n",
      "19/11/08 02:52:50 INFO spark.ContextCleaner: Cleaned accumulator 220\n",
      "19/11/08 02:52:50 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:44147 in memory (size: 6.8 KB, free: 282.3 MB)\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 203\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 212\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 223\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 224\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 205\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 213\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 208\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 217\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 211\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 218\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 221\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 210\n",
      "19/11/08 02:52:51 INFO spark.ContextCleaner: Cleaned accumulator 225\n",
      "19/11/08 02:52:55 INFO python.PythonRunner: Times: total = 5914, boot = -4904, init = 4910, finish = 5908\n",
      "19/11/08 02:52:55 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 34). 1552 bytes result sent to driver\n",
      "19/11/08 02:52:55 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 35, localhost, executor driver, partition 2, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:52:55 INFO executor.Executor: Running task 2.0 in stage 11.0 (TID 35)\n",
      "19/11/08 02:52:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 34) in 5961 ms on localhost (executor driver) (1/4)\n",
      "19/11/08 02:52:55 INFO storage.BlockManager: Found block rdd_1_2 locally\n",
      "19/11/08 02:53:12 INFO python.PythonRunner: Times: total = 23140, boot = -98, init = 104, finish = 23134\n",
      "19/11/08 02:53:12 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 33). 1552 bytes result sent to driver\n",
      "19/11/08 02:53:12 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 11.0 (TID 36, localhost, executor driver, partition 3, PROCESS_LOCAL, 7921 bytes)\n",
      "19/11/08 02:53:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 33) in 23189 ms on localhost (executor driver) (2/4)\n",
      "19/11/08 02:53:12 INFO executor.Executor: Running task 3.0 in stage 11.0 (TID 36)\n",
      "19/11/08 02:53:13 INFO storage.BlockManager: Found block rdd_1_3 locally\n",
      "19/11/08 02:53:19 INFO python.PythonRunner: Times: total = 23292, boot = -3, init = 5, finish = 23290\n",
      "19/11/08 02:53:19 INFO executor.Executor: Finished task 2.0 in stage 11.0 (TID 35). 1552 bytes result sent to driver\n",
      "19/11/08 02:53:19 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 35) in 23302 ms on localhost (executor driver) (3/4)\n",
      "19/11/08 02:53:23 INFO python.PythonRunner: Times: total = 10607, boot = -4, init = 7, finish = 10604\n",
      "19/11/08 02:53:23 INFO executor.Executor: Finished task 3.0 in stage 11.0 (TID 36). 1552 bytes result sent to driver\n",
      "19/11/08 02:53:23 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 11.0 (TID 36) in 10617 ms on localhost (executor driver) (4/4)\n",
      "19/11/08 02:53:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "19/11/08 02:53:23 INFO scheduler.DAGScheduler: ResultStage 11 (reduce at /home/jovyan/nasa_requests.py:103) finished in 33.817 s\n",
      "19/11/08 02:53:23 INFO scheduler.DAGScheduler: Job 6 finished: reduce at /home/jovyan/nasa_requests.py:103, took 33.823064 s\n",
      "\n",
      "O número de hosts únicos: 137978\n",
      "\n",
      "O total de erros 404: 20901\n",
      "\n",
      "As 5 URL's que mais causaram erro 404:\n",
      "URL: /pub/winvn/readme.txt Quantidade: 2004\n",
      "URL: /pub/winvn/release.txt Quantidade: 1732\n",
      "URL: /shuttle/missions/STS-69/mission-STS-69.html Quantidade: 682\n",
      "URL: /shuttle/missions/sts-68/ksc-upclose.gif Quantidade: 426\n",
      "URL: /history/apollo/a-001/a-001-patch-small.gif Quantidade: 384\n",
      "\n",
      "A quantidade de erros 404 por dia: \n",
      "Dia: 01/Aug/1995 Quantidade: 243\n",
      "Dia: 03/Aug/1995 Quantidade: 304\n",
      "Dia: 04/Aug/1995 Quantidade: 346\n",
      "Dia: 05/Aug/1995 Quantidade: 236\n",
      "Dia: 06/Aug/1995 Quantidade: 373\n",
      "Dia: 07/Aug/1995 Quantidade: 537\n",
      "Dia: 08/Aug/1995 Quantidade: 391\n",
      "Dia: 09/Aug/1995 Quantidade: 279\n",
      "Dia: 10/Aug/1995 Quantidade: 315\n",
      "Dia: 11/Aug/1995 Quantidade: 263\n",
      "Dia: 12/Aug/1995 Quantidade: 196\n",
      "Dia: 13/Aug/1995 Quantidade: 216\n",
      "Dia: 14/Aug/1995 Quantidade: 287\n",
      "Dia: 15/Aug/1995 Quantidade: 327\n",
      "Dia: 16/Aug/1995 Quantidade: 259\n",
      "Dia: 17/Aug/1995 Quantidade: 271\n",
      "Dia: 18/Aug/1995 Quantidade: 256\n",
      "Dia: 19/Aug/1995 Quantidade: 209\n",
      "Dia: 20/Aug/1995 Quantidade: 312\n",
      "Dia: 21/Aug/1995 Quantidade: 305\n",
      "Dia: 22/Aug/1995 Quantidade: 288\n",
      "Dia: 23/Aug/1995 Quantidade: 345\n",
      "Dia: 24/Aug/1995 Quantidade: 420\n",
      "Dia: 25/Aug/1995 Quantidade: 415\n",
      "Dia: 26/Aug/1995 Quantidade: 366\n",
      "Dia: 27/Aug/1995 Quantidade: 370\n",
      "Dia: 28/Aug/1995 Quantidade: 410\n",
      "Dia: 29/Aug/1995 Quantidade: 420\n",
      "Dia: 30/Aug/1995 Quantidade: 571\n",
      "Dia: 31/Aug/1995 Quantidade: 526\n",
      "Dia: 01/Jul/1995 Quantidade: 316\n",
      "Dia: 02/Jul/1995 Quantidade: 291\n",
      "Dia: 03/Jul/1995 Quantidade: 474\n",
      "Dia: 04/Jul/1995 Quantidade: 359\n",
      "Dia: 05/Jul/1995 Quantidade: 497\n",
      "Dia: 06/Jul/1995 Quantidade: 640\n",
      "Dia: 07/Jul/1995 Quantidade: 570\n",
      "Dia: 08/Jul/1995 Quantidade: 302\n",
      "Dia: 09/Jul/1995 Quantidade: 348\n",
      "Dia: 10/Jul/1995 Quantidade: 398\n",
      "Dia: 11/Jul/1995 Quantidade: 471\n",
      "Dia: 12/Jul/1995 Quantidade: 471\n",
      "Dia: 13/Jul/1995 Quantidade: 532\n",
      "Dia: 14/Jul/1995 Quantidade: 413\n",
      "Dia: 15/Jul/1995 Quantidade: 254\n",
      "Dia: 16/Jul/1995 Quantidade: 257\n",
      "Dia: 17/Jul/1995 Quantidade: 406\n",
      "Dia: 18/Jul/1995 Quantidade: 465\n",
      "Dia: 19/Jul/1995 Quantidade: 639\n",
      "Dia: 20/Jul/1995 Quantidade: 428\n",
      "Dia: 21/Jul/1995 Quantidade: 334\n",
      "Dia: 22/Jul/1995 Quantidade: 192\n",
      "Dia: 23/Jul/1995 Quantidade: 233\n",
      "Dia: 24/Jul/1995 Quantidade: 328\n",
      "Dia: 25/Jul/1995 Quantidade: 461\n",
      "Dia: 26/Jul/1995 Quantidade: 336\n",
      "Dia: 27/Jul/1995 Quantidade: 336\n",
      "Dia: 28/Jul/1995 Quantidade: 94\n",
      "\n",
      "A quantidade de bytes utilizados: 65524314915\n",
      "\n",
      "19/11/08 02:53:23 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "19/11/08 02:53:23 INFO server.AbstractConnector: Stopped Spark@3d868c80{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "19/11/08 02:53:23 INFO ui.SparkUI: Stopped Spark web UI at http://jupyter-lucas91batista-2dnasa-2dlog-2danalysis-2dh9rre0s5:4040\n",
      "19/11/08 02:53:23 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/11/08 02:53:23 INFO memory.MemoryStore: MemoryStore cleared\n",
      "19/11/08 02:53:23 INFO storage.BlockManager: BlockManager stopped\n",
      "19/11/08 02:53:23 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/11/08 02:53:23 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/11/08 02:53:23 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "19/11/08 02:53:23 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "19/11/08 02:53:23 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ba0f38e3-2b81-4484-ae30-4bdf2ed0c3a4/pyspark-6f910e42-6091-43b3-b0ec-3d0e6fe86f40\n",
      "19/11/08 02:53:23 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9ff3b6b8-1244-456a-bf45-453e4ba77fda\n",
      "19/11/08 02:53:23 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ba0f38e3-2b81-4484-ae30-4bdf2ed0c3a4\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master yarn nasa_requests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Teoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qual o objetivo do comando cache em Spark?\n",
    "O comando cache armazena o RDD (Resilient Distributed Dataset) em memória. Utilizamos a função cache quando queremos reutilizar o RDD. O RDD é reprocessado cada vez que executamos uma ação, utilizando a função cache este RDD ficará disponível na memória aumentando o desempenho de acesso em seu reuso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?\n",
    "O Spark utiliza o conceito de DAG (Directed Acyclic Graph - Grafos Acíclicos Direcionados) que permite mapear os passos que serão necessários para atingir o resultado esperado e, então, otimiza esses passos da melhor maneira possível; O Spark faz suas operações em memória; O Spark trabalha com RDD que armazena os elementos, é tolerante a falhas e pode ser executado em paralelo. Além disso, o Spark não computa os seus resultados de imediato, uma transformação é executada apenas quando uma ação é solicitada, isto também contribui para o seu desempenho. Por estes motivos, o Spark consegue executar um trabalho de maneira mais eficiente que o MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qual é a função do SparkContext?\n",
    "O SparkContext permite a conexão com um cluster Spark e então podemos utilizar os recursos do Spark naquele cluster, por exemplo criar RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explique com suas palavras o que é Resilient Distributed Datasets (RDD).\n",
    "O RDD é a principal estrutura de dados do Spark, é um conjunto de registros imutáveis que podem ser executados em paralelo. É possível realizar transformações e ações no RDD. Apenas as transformações geram um novo RDD e essas transformações só são aplicadas quando uma ação é necessária. Cada vez que é necessário utilizar um RDD ele é reprocessado, mas existem funções para persistir um RDD em memória ou disco melhorando o desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?\n",
    "O GroupByKey é menos eficiente que o reduceByKey porque ele transfere mais dados pela rede para computar o resultado final. O GroupByKey percorre todas as partições em busca de todos os valores para todas as chaves e então transfere todos esses dados via rede para obter o resultado final (processo conhecido como shuffle). Por outro lado, o reduceByKey primeiro agrega todas os valores das chaves, transferindo via rede uma menor quantidade de dados, ou seja, apenas um valor para cada chave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explique o que o código Scala abaixo faz.\n",
    "---\n",
    "``` scala\n",
    "val​​ ​ textFile​​ ​ = ​ ​ sc​ . ​ textFile​ ( ​ \"hdfs://...\"​ )\n",
    "val​​ ​ counts​​ ​ = ​ ​ textFile​ . ​ flatMap​ ( ​ line​​ ​ =>​​ ​ line​ . ​ split​ ( ​ \" ​ ​ \" ​ ))\n",
    "​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ . ​ map​ ( ​ word​​ ​ =>​​ ​ ( ​ word​ , ​ ​ 1 ​ ))\n",
    "​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ . ​ reduceByKey​ ( ​ _ ​ ​ + ​ ​ _ ​ )\n",
    "counts​ . ​ saveAsTextFile​ ( ​ \"hdfs://...\"​ )\n",
    "``` \n",
    "---\n",
    "\n",
    "O código realiza a contagem de palavras de um arquivo lido do HDFS e salva seu resultado em um arquivo texto também no HDFS. É criado um RDD a partir do arquivo lido do HDFS, em seguida ocorrem várias transformações: o RDD inicial é transformado utilizando a função flatMap, assim obtemos as palavras separando cada entrada por espaço. Em seguida, transformamos o RDD utilizando a função map para exportar pares chave-valor com a palavras sendo a chave e o valor o numeral 1. Por fim, a transformação reduceBykey irá somar os valores para cada chave. A ação saveAsTextFile faz com que as transformações sejam de fato processadas e salva o resultado em um arquivo no HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
