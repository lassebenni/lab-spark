{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import findspark\nfindspark.init()\nimport pyspark","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Steps can be found on https://docs.delta.io/latest/quick-start.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from delta.tables import *","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create a table\n\nTo create a Delta table, write a DataFrame out in the delta format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"/tmp/delta-table\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These operations create a new Delta table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta table, see Create a table and Write to a table."},{"metadata":{},"cell_type":"markdown","source":"# Read data\n\nYou read data in your Delta table by specifying the path to the files: \"/tmp/delta-table\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Update table data\n\nDelta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overwrite\ndata = spark.range(5, 10)\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you read this table again, you should see only the values 5-9 you have added because you overwrote the previous data.m"},{"metadata":{},"cell_type":"markdown","source":"# Conditional update without overwrite\n\nDelta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. Here are a few examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"from delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n\n# Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = { \"id\": expr(\"id + 100\") })\n\n# Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n# Upsert (merge) new data\nnewData = spark.range(0, 20)\n\ndeltaTable.alias(\"oldData\") \\\n  .merge(\n    newData.alias(\"newData\"),\n    \"oldData.id = newData.id\") \\\n  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n  .execute()\n\ndeltaTable.toDF().show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should see that some of the existing rows have been updated and new rows have been inserted."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}